Hereâ€™s a simple distributed application using Hadoop MapReduce to process a system log file and count the number of visits per URL:

---

## **1. Problem Statement**

Given a log file where each line represents a request (e.g., web server logs), count the total number of visits to each URL.

---

## **2. MapReduce Application Outline**

- **Input:** Log file stored in HDFS, each line contains fields like timestamp, IP, URL, etc.
- **Output:** For each URL, the total number of visits.

---

## **3. Mapper Logic**

- Read each log line.
- Parse the URL field.
- Emit `` for each occurrence[5][4].

---

## **4. Reducer Logic**

- For each URL key, sum all the counts.
- Emit ``[5][4].

---

## **5. Sample Java Code**

```java
public class LogProcessor {
  public static class LogMapper extends Mapper {
    private final static IntWritable one = new IntWritable(1);
    private Text url = new Text();

    public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
      String[] fields = value.toString().split(" "); // Adjust delimiter as per your log format
      if(fields.length > 2) {
        url.set(fields[2]); // Assuming URL is the third field
        context.write(url, one);
      }
    }
  }

  public static class SumReducer extends Reducer {
    public void reduce(Text key, Iterable values, Context context) throws IOException, InterruptedException {
      int sum = 0;
      for(IntWritable val : values) {
        sum += val.get();
      }
      context.write(key, new IntWritable(sum));
    }
  }
}
```

---

## **6. Run the Application**

- Compile and package as a JAR.
- Upload your log file to HDFS.
- Run with:  
  ```
  hadoop jar logprocessor.jar LogProcessor /input/logfile /output/result
  ```
- Output: Each line shows a URL and its visit count[5][1].

---


To compile your Hadoop MapReduce Java program into a JAR file, follow these detailed steps:

---

## **1. Prepare Your Java Source File**

- Save your code (e.g., `LogProcessor.java`) in a directory, say `logprocessor`.

---

## **2. Locate Hadoop JARs**

- Hadoop libraries (needed for compilation) are typically in `$HADOOP_HOME/share/hadoop/common/` and `$HADOOP_HOME/share/hadoop/mapreduce/`[3][6].

---

## **3. Compile the Java Code**

Open a terminal, navigate to your source directory, and run:

```bash
javac -classpath "$HADOOP_HOME/share/hadoop/common/*:$HADOOP_HOME/share/hadoop/mapreduce/*" -d logprocessor_classes LogProcessor.java
```
- `-classpath` tells the compiler where to find Hadoop classes.
- `-d logprocessor_classes` puts compiled `.class` files in the `logprocessor_classes` directory[6][8].

---

## **4. Create the JAR File**

Package the compiled classes into a JAR:

```bash
jar -cvf logprocessor.jar -C logprocessor_classes/ .
```
- This creates `logprocessor.jar` containing your application[6][8].

---

## **5. Run the MapReduce Job**

Upload your input file to HDFS, then run:

```bash
hadoop jar logprocessor.jar LogProcessor /input/path /output/path
```

---

**Summary Table**

| Step        | Command Example                                                                 |
|-------------|--------------------------------------------------------------------------------|
| Compile     | `javac -classpath "$HADOOP_HOME/share/hadoop/common/*:$HADOOP_HOME/share/hadoop/mapreduce/*" -d logprocessor_classes LogProcessor.java` |
| Create JAR  | `jar -cvf logprocessor.jar -C logprocessor_classes/ .`                          |
| Run Job     | `hadoop jar logprocessor.jar LogProcessor /input/path /output/path`             |

---

